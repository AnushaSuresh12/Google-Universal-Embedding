{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## I hope that this notebook is helpful to enjoy the competition.\n\n## Model\n- for training:  \nbackbone(CLIP) + Dropout + Dense(units=256) + Arcface + Softmax (classes=17691)\n- for inference:  \nbackbone(CLIP) + Dropout + Dense(units=256) + AdaptiveAveragePooling(n=64)\n\nNote:  \n[Discussion](https://www.kaggle.com/competitions/google-universal-image-embedding/discussion/337384#1908364)  \nThe CLIP in TensorFlow cannot work well on kaggle TPU notebook.  \nIf you train it with TPU, I recommend to implement on google colab.\n\n## Dataset for training:  \n- [imagenet1k](https://www.kaggle.com/datasets/motono0223/guie-imagenet1k-mini1-tfrecords-label-0-999)  \n  This dataset was created from imagenet(1k) dataset.  \n  To reduce the dataset size, this dataset has only 50 images per class.  \n\n- [products10k](https://www.kaggle.com/datasets/motono0223/guie-products10k-tfrecords-label-1000-10690)  \n  This dataset was created from [the product10k dataset](https://products-10k.github.io/).   \n  To reduce the dataset size, this dataset has only 50 images per class.  \n\n- [google landmark recognition 2021(Competition dataset)](https://www.kaggle.com/datasets/motono0223/guie-glr2021mini-tfrecords-label-10691-17690)  \n  This dataset was created from [the competition dataset](https://www.kaggle.com/competitions/landmark-recognition-2021/data).  \n  To reduce the dataset size, this dataset uses the top 7k class images with a large number of images (50 images per class).  \n","metadata":{}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"cUF4H1xBsYb6"}},{"cell_type":"code","source":"import os\ndef is_colab_env():\n    is_colab = False\n    for k in os.environ.keys():\n        if \"COLAB\" in k:\n            is_colab = True\n            break\n    return is_colab\n\n# if google colab, install transformers and tensorflow_addons\n# (Note: please use google colab(TPU) when model is trained. \n#  On the kaggle TPU env, the module transformers.TFCLIPVisionModel couldn't be installed.)\nif is_colab_env():\n    !pip install transformers\n    !pip install tensorflow_addons","metadata":{"id":"05oZ1Q5Idhj-","outputId":"548ec03d-91de-4a68-bfb9-9830ea763b5e","execution":{"iopub.status.busy":"2022-10-10T12:54:37.818529Z","iopub.execute_input":"2022-10-10T12:54:37.81899Z","iopub.status.idle":"2022-10-10T12:54:37.84626Z","shell.execute_reply.started":"2022-10-10T12:54:37.818889Z","shell.execute_reply":"2022-10-10T12:54:37.84515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import CLIPProcessor, TFCLIPVisionModel, CLIPFeatureExtractor\n\nimport re\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport random\nimport math\nimport tensorflow as tf\ntf.keras.backend.set_floatx('float16')\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, train_test_split, StratifiedKFold\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import normalize\nimport pickle\nimport json\nimport tensorflow_hub as tfhub\nfrom datetime import datetime\nimport gc\nimport requests\nfrom mpl_toolkits import axes_grid1","metadata":{"id":"i72153AaDJds","execution":{"iopub.status.busy":"2022-10-10T12:54:37.849756Z","iopub.execute_input":"2022-10-10T12:54:37.850028Z","iopub.status.idle":"2022-10-10T12:54:50.609731Z","shell.execute_reply.started":"2022-10-10T12:54:37.850004Z","shell.execute_reply":"2022-10-10T12:54:50.608786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Device","metadata":{"id":"k8BdwSO1sYcN"}},{"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"id":"khrTPhLcR39a","outputId":"7adb1df9-4f27-4042-c084-cfc7881b8728","execution":{"iopub.status.busy":"2022-10-10T12:54:50.611241Z","iopub.execute_input":"2022-10-10T12:54:50.611765Z","iopub.status.idle":"2022-10-10T12:54:50.621416Z","shell.execute_reply.started":"2022-10-10T12:54:50.611735Z","shell.execute_reply":"2022-10-10T12:54:50.620168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class config:\n    VERSION = 3\n    SUBV = \"Clip_ViT_Train\"\n\n    SEED = 42\n\n    # pretrained model\n    RESUME = True\n    RESUME_EPOCH = 0\n    RESUME_WEIGHT = \"../input/finalmodels/clip-vit-large-patch14_224pix-emb256_loss.h5\"\n\n    # backbone model\n    model_type = \"clip-vit-large-patch14\"\n    EFF_SIZE = 0\n    EFF2_TYPE = \"\"\n    IMAGE_SIZE = 224\n\n    # projection layer\n    N_CLASSES = 92004+101+196+1\n    EMB_DIM = 256  # = 64 x N\n    \n    # training\n    TRAIN = False\n    BATCH_SIZE = 200 * strategy.num_replicas_in_sync\n    EPOCHS = 100\n    LR = 0.001\n    save_dir = \"./\"\n\n    DEBUG = False\n    \n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \n# model name\nMODEL_NAME = None\nif config.model_type == 'effnetv1':\n    MODEL_NAME = f'effnetv1_b{config.EFF_SIZE}'\nelif config.model_type == 'effnetv2':\n    MODEL_NAME = f'effnetv2_{config.EFF2_TYPE}'\nelif \"swin\" in config.model_type:\n    MODEL_NAME = config.model_type\nelif \"conv\" in config.model_type:\n    MODEL_NAME = config.model_type\nelse:\n    MODEL_NAME = config.model_type\nconfig.MODEL_NAME = MODEL_NAME\nprint(MODEL_NAME)","metadata":{"id":"lCwQB_L1NGoH","outputId":"040dbaa6-54e6-4135-8251-bd779314b05f","execution":{"iopub.status.busy":"2022-10-10T12:54:50.624856Z","iopub.execute_input":"2022-10-10T12:54:50.625244Z","iopub.status.idle":"2022-10-10T12:54:50.635421Z","shell.execute_reply.started":"2022-10-10T12:54:50.625214Z","shell.execute_reply":"2022-10-10T12:54:50.634293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TFRecords","metadata":{"id":"FaUn2_W4Hm3t"}},{"cell_type":"markdown","source":"# Viz tfrecord images","metadata":{"id":"EbjJ86S4sYci"}},{"cell_type":"markdown","source":"# Model","metadata":{"id":"dOPX4LshNXsM"}},{"cell_type":"code","source":"# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https://arxiv.org/pdf/1801.07698.pdf\n        https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution/\n            blob/master/src/modeling/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output","metadata":{"id":"1LjaDRLgMjdq","execution":{"iopub.status.busy":"2022-10-10T12:54:50.637311Z","iopub.execute_input":"2022-10-10T12:54:50.637672Z","iopub.status.idle":"2022-10-10T12:54:50.65553Z","shell.execute_reply.started":"2022-10-10T12:54:50.637634Z","shell.execute_reply":"2022-10-10T12:54:50.654545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_scale_layer(rescale_mode = \"tf\"):\n    # For keras_cv_attention_models module:\n    # ref: https://github.com/leondgarse/keras_cv_attention_models/blob/main/keras_cv_attention_models/imagenet/data.py\n    # ref function : init_mean_std_by_rescale_mode()\n\n    # For effV2 (21k classes) : https://github.com/leondgarse/keras_efficientnet_v2\n\n    if isinstance(rescale_mode, (list, tuple)):  # Specific mean and std\n        mean, std = rescale_mode\n    elif rescale_mode == \"torch\":\n        mean = np.array([0.485, 0.456, 0.406]) * 255.0\n        std = np.array([0.229, 0.224, 0.225]) * 255.0\n    elif rescale_mode == \"tf\":  # [0, 255] -> [-1, 1]\n        mean, std = 127.5, 127.5\n    elif rescale_mode == \"tf128\":  # [0, 255] -> [-1, 1]\n        mean, std = 128.0, 128.0\n    elif rescale_mode == \"raw01\":\n        mean, std = 0, 255.0  # [0, 255] -> [0, 1]\n    else:\n        mean, std = 0, 1  # raw inputs [0, 255]        \n    scaling_layer = keras.layers.Lambda(lambda x: ( tf.cast(x, tf.float32) - mean) / std )\n    \n    return scaling_layer\n\n\ndef get_clip_model():\n    inp = tf.keras.layers.Input(shape = [3, 224, 224]) # [B, C, H, W]\n    backbone = TFCLIPVisionModel.from_pretrained(\"laion/CLIP-ViT-g-14-laion2B-s12B-b42K\",from_pt=True)\n    output = backbone({'pixel_values':inp}).pooler_output\n    return tf.keras.Model(inputs=[inp], outputs=[output])\n\ndef get_embedding_model():\n    #------------------\n    # Definition of placeholders\n    inp = tf.keras.layers.Input(shape = [None, None, 3], name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n\n    # Definition of layers\n    layer_resize = tf.keras.layers.Lambda(lambda x: tf.image.resize(x, [config.IMAGE_SIZE, config.IMAGE_SIZE]), name='resize')\n    layer_scaling = get_scale_layer(rescale_mode = \"torch\")\n    layer_permute = tf.keras.layers.Permute((3,1,2))\n    layer_backbone = get_clip_model()\n    layer_dropout = tf.keras.layers.Dropout(0.2)\n    layer_dense_before_arcface = tf.keras.layers.Dense(config.EMB_DIM)\n    layer_margin = ArcMarginProduct(\n        n_classes = 92302, \n        s = 30, \n        m = 0.3, \n        name=f'head/arcface', \n        dtype='float32'\n        )\n    layer_softmax = tf.keras.layers.Softmax(dtype='float32')\n    layer_l2 = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=-1), name='embedding_norm')\n    \n    if config.EMB_DIM != 64:\n        layer_adaptive_pooling = tfa.layers.AdaptiveAveragePooling1D(64)\n    else:\n        layer_adaptive_pooling = tf.keras.layers.Lambda(lambda x: x )  # layer with no operation\n\n    #------------------\n    # Definition of entire model\n    image = layer_scaling(inp)\n    image = layer_resize(image)\n    image = layer_permute(image)\n    backbone_output = layer_backbone(image)\n    embed = layer_dropout(backbone_output)\n    embed = layer_dense_before_arcface(embed)\n    x = layer_margin([embed, label])\n    output = layer_softmax(x)\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output]) # whole architecture\n\n    #model.layers[-6].trainable = False\n    opt = tf.keras.optimizers.Adam(learning_rate = config.LR)\n    model.compile(\n        optimizer = opt,\n        loss = [tf.keras.losses.SparseCategoricalCrossentropy()],\n        metrics = [tf.keras.metrics.SparseCategoricalAccuracy(),tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5)]\n        )\n\n    #------------------\n    # Definition of embedding model (for submission)\n    embed_model = keras.Sequential([\n        keras.layers.InputLayer(input_shape=(None, None, 3), dtype='uint8'),\n        layer_scaling,\n        layer_resize,\n        layer_permute,\n        layer_backbone,\n        layer_dropout,\n        layer_dense_before_arcface,\n        layer_adaptive_pooling,    # shape:[None, config.EMB_DIM] --> [None, 64]\n        layer_l2,\n    ])\n\n\n    return model, embed_model","metadata":{"id":"jX76WJYoMgey","execution":{"iopub.status.busy":"2022-10-10T12:54:50.65707Z","iopub.execute_input":"2022-10-10T12:54:50.657492Z","iopub.status.idle":"2022-10-10T12:54:50.677717Z","shell.execute_reply.started":"2022-10-10T12:54:50.657457Z","shell.execute_reply":"2022-10-10T12:54:50.676727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model, emb_model = get_embedding_model()\n\nif config.RESUME:\n    print(f\"load {config.RESUME_WEIGHT}\")\n    model.load_weights( config.RESUME_WEIGHT )\n    #emb_model.load_weights( config.RESUME_WEIGHT )","metadata":{"id":"5nGM8XncMglt","outputId":"a8526809-ce45-4b10-f642-a72c6014cb1e","execution":{"iopub.status.busy":"2022-10-10T12:54:50.680511Z","iopub.execute_input":"2022-10-10T12:54:50.681779Z","iopub.status.idle":"2022-10-10T12:56:24.130424Z","shell.execute_reply.started":"2022-10-10T12:54:50.681734Z","shell.execute_reply":"2022-10-10T12:56:24.12881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"qSWkPesHMgpO","outputId":"b65d4347-69b9-4d35-fcc4-02cf8e1d68b7","execution":{"iopub.status.busy":"2022-10-10T12:56:24.138022Z","iopub.execute_input":"2022-10-10T12:56:24.142037Z","iopub.status.idle":"2022-10-10T12:56:24.246097Z","shell.execute_reply.started":"2022-10-10T12:56:24.141975Z","shell.execute_reply":"2022-10-10T12:56:24.244564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emb_model.summary()","metadata":{"id":"0a5LW0tnMgsX","outputId":"9143889a-8d13-493d-bb8a-f0323a7da97f","execution":{"iopub.status.busy":"2022-10-10T12:56:24.252239Z","iopub.execute_input":"2022-10-10T12:56:24.255751Z","iopub.status.idle":"2022-10-10T12:56:24.326658Z","shell.execute_reply.started":"2022-10-10T12:56:24.255685Z","shell.execute_reply":"2022-10-10T12:56:24.325568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Scheduler","metadata":{"id":"5z_2vp1TsYcx"}},{"cell_type":"markdown","source":"# Train entire model ","metadata":{"id":"CDmefQyfsYcz"}},{"cell_type":"code","source":"# save for debug\n#emb_model.save_weights( config.save_dir+f\"/{config.MODEL_NAME}_{config.IMAGE_SIZE}pix-emb{config.EMB_DIM}_emb_model.h5\" )","metadata":{"execution":{"iopub.status.busy":"2022-10-10T12:56:24.330932Z","iopub.execute_input":"2022-10-10T12:56:24.331772Z","iopub.status.idle":"2022-10-10T12:56:24.336758Z","shell.execute_reply.started":"2022-10-10T12:56:24.331729Z","shell.execute_reply":"2022-10-10T12:56:24.335663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create submission.zip","metadata":{"id":"kG-r1le7SF9F"}},{"cell_type":"code","source":"save_locally = tf.saved_model.SaveOptions(\n    experimental_io_device='/job:localhost'\n)\nemb_model.save('./embedding_norm_model', options=save_locally)","metadata":{"execution":{"iopub.status.busy":"2022-10-10T12:56:24.338397Z","iopub.execute_input":"2022-10-10T12:56:24.33877Z","iopub.status.idle":"2022-10-10T12:58:42.558799Z","shell.execute_reply.started":"2022-10-10T12:56:24.338732Z","shell.execute_reply":"2022-10-10T12:58:42.557647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ndel emb_model\ndel model\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-10-10T12:58:42.572978Z","iopub.execute_input":"2022-10-10T12:58:42.573611Z","iopub.status.idle":"2022-10-10T12:58:46.702923Z","shell.execute_reply.started":"2022-10-10T12:58:42.573565Z","shell.execute_reply":"2022-10-10T12:58:46.701893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfrom zipfile import ZipFile\n\nwith ZipFile('submission.zip','w') as zip:           \n    zip.write(\n        './embedding_norm_model/saved_model.pb', \n        arcname='saved_model.pb'\n    ) \n    zip.write(\n        './embedding_norm_model/variables/variables.data-00000-of-00001', \n        arcname='variables/variables.data-00000-of-00001'\n    ) \n    zip.write(\n        './embedding_norm_model/variables/variables.index', \n        arcname='variables/variables.index'\n    )","metadata":{"id":"_eeqo14RMxEr","execution":{"iopub.status.busy":"2022-10-10T12:58:46.704499Z","iopub.execute_input":"2022-10-10T12:58:46.705219Z","iopub.status.idle":"2022-10-10T12:58:59.536214Z","shell.execute_reply.started":"2022-10-10T12:58:46.705177Z","shell.execute_reply":"2022-10-10T12:58:59.535077Z"},"trusted":true},"execution_count":null,"outputs":[]}]}